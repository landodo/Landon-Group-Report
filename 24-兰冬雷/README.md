# SENet å’Œå®ƒçš„å­ªç”Ÿå…„å¼Ÿ SKNet

âœ… è®ºæ–‡åœ°å€ï¼š

- Squeeze-and-Excitation Networks: [https://arxiv.org/pdf/1709.01507.pdf](<https://arxiv.org/pdf/1709.01507.pdf>)
- Selective Kernel Networks: [https://arxiv.org/pdf/1903.06586.pdf](<https://arxiv.org/pdf/1903.06586.pdf>)

âœ… è®ºæ–‡å‘è¡¨æ—¶é—´ï¼ˆarXiv V1ï¼‰

- SENetï¼š2017 å¹´ 9 æœˆ 5 æ—¥
- SKNetï¼š2019 å¹´ 3 æœˆ 15 æ—¥

## ç›¸å…³çš„è®ºæ–‡

ï¼ˆ1ï¼‰Inception ç³»åˆ—ï¼ˆ2014 å¹´~2016 å¹´ï¼‰ï¼šInception ç»“æ„ä¸­åµŒå…¥äº†å¤šå°ºåº¦ä¿¡æ¯ï¼Œèšåˆå¤šç§ä¸åŒæ„Ÿå—é‡ä¸Šçš„ç‰¹å¾æ¥è·å¾—æ€§èƒ½å¢ç›Šã€‚

- Inception V1 (GoogLeNet): 11 Sep 2014
- Inception V2 (Batch Normalization): 11 Feb 2015
- Inception V3: 2 Dec 2015
- Inception V4: 23 Feb 2016
- Xception: 7 Oct 2016

ï¼ˆ2ï¼‰ResNet ï¼ˆ10 Dec 2015ï¼‰

ï¼ˆ3ï¼‰ResNeXtï¼ˆ16 Nov 2016ï¼‰

ï¼ˆ4ï¼‰Inside-Outside Networkï¼ˆ14 Dec 2015ï¼‰ï¼šç½‘ç»œä¸­è€ƒè™‘äº†ç©ºé—´ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

ï¼ˆ5ï¼‰Spatial Transform Networkï¼ˆ5 Jun 2015ï¼‰ï¼šAttention æœºåˆ¶å¼•å…¥åˆ°ç©ºé—´ç»´åº¦ã€‚

-   - Dynamic Capacity Networkï¼ˆ24 Nov 2015ï¼‰

**ï¼ˆ6ï¼‰ğŸ’¢ SENetï¼ˆ5 Sep 2017ï¼‰ï¼šé€šé“æ³¨æ„åŠ›**

ï¼ˆ7ï¼‰CBAMï¼ˆ17 Jul 2018ï¼‰ï¼šç©ºé—´æ³¨æ„åŠ›+é€šé“æ³¨æ„åŠ›ç›¸ç»“åˆ

**ï¼ˆ8ï¼‰ğŸ’¢SKNetï¼ˆ15 Mar 2019ï¼‰**

## SENet

> ä¸€ä½œï¼šèƒ¡æ°ï¼Œå…³äº SENet ä¸­æ–‡ä»‹ç»ï¼š
>
> - Momenta è¯¦è§£ ImageNet 2017 å¤ºå† æ¶æ„ SENet  [https://www.sohu.com/a/161633191_465975](<https://www.sohu.com/a/161633191_465975>)

![](./20210122/1.png)

ğŸŒ€é€šé“é—´çš„ç‰¹å¾éƒ½æ˜¯å¹³ç­‰çš„å—ï¼ŸSENet ç»™å‡ºäº†è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚

è®ºæ–‡çš„ä¸»è¦å·¥ä½œæ˜¯ï¼šè€ƒè™‘ç‰¹å¾é€šé“ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºäº† Squeeze-and-Excitation Networksï¼ˆç®€ç§° SENetï¼‰ã€‚æ˜¾å¼åœ°å»ºæ¨¡ç‰¹å¾é€šé“ä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ï¼Œé€šè¿‡å­¦ä¹ çš„æ–¹å¼æ¥è‡ªåŠ¨è·å–åˆ°æ¯ä¸ªç‰¹å¾é€šé“çš„é‡è¦ç¨‹åº¦ï¼Œç„¶åä¾ç…§è¿™ä¸ªé‡è¦ç¨‹åº¦å»æå‡æœ‰ç”¨çš„ç‰¹å¾å¹¶æŠ‘åˆ¶å¯¹å½“å‰ä»»åŠ¡ç”¨å¤„ä¸å¤§çš„ç‰¹å¾ã€‚

SE block å¦‚ä¸‹ Fig. 1 æ‰€ç¤ºã€‚

![](./20210122/2.PNG)

### SQUEEZE-AND-EXCITATION BLOCKS

Squeeze å’Œ Excitation æ˜¯ä¸¤ä¸ªéå¸¸å…³é”®çš„æ“ä½œã€‚

ç»™å®šä¸€ä¸ªè¾“å…¥ $X$ï¼Œ$X \in  \mathbb{R}^{C' \times H' \times W'}$ï¼Œé€šè¿‡ä¸€ç³»åˆ—å·ç§¯ç­‰ä¸€èˆ¬å˜æ¢ $F_{tr}$ åï¼Œå¾—åˆ°ä¸€ä¸ª $U \in \mathbb{R}^{C \times H \times W}$ çš„ç‰¹å¾å›¾ã€‚

æ¥ä¸‹æ¥é€šè¿‡ä¸€ä¸ª Squeeze and Excitation block ï¼Œä¸‰ä¸ªæ“ä½œæ¥é‡æ ‡å®šå‰é¢å¾—åˆ°çš„ç‰¹å¾ã€‚

ï¼ˆ1ï¼‰Squeeze: $F_{sq}(\cdot)$

é¦–å…ˆæ˜¯ Squeeze æ“ä½œï¼Œé¡ºç€ç©ºé—´ç»´åº¦æ¥è¿›è¡Œç‰¹å¾å‹ç¼©ï¼Œå°†æ¯ä¸ªäºŒç»´çš„ç‰¹å¾é€šé“å˜æˆä¸€ä¸ªå®æ•°ï¼Œè¿™ä¸ªå®æ•°æŸç§ç¨‹åº¦ä¸Šå…·æœ‰å…¨å±€çš„æ„Ÿå—é‡ï¼Œå¹¶ä¸”è¾“å‡ºçš„ç»´åº¦å’Œè¾“å…¥çš„ç‰¹å¾é€šé“æ•°ç›¸åŒ¹é…ã€‚

å³ï¼šå¯¹ $C \times H \times W$ çš„ç‰¹å¾å›¾è¿›è¡Œ global average poolingï¼Œå¾—åˆ° $1 \times 1 \times C$ çš„ç‰¹å¾å›¾ã€‚

$$z_c = F_{sq}(u_c) = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_c{(i, j)}$$

ï¼ˆ2ï¼‰Excitation: $F_{ex}(\cdot , W)$

é€šè¿‡å‚æ•° W æ¥ä¸ºæ¯ä¸ªç‰¹å¾é€šé“ç”Ÿæˆæƒé‡ï¼Œå…¶ä¸­å‚æ•° W è¢«å­¦ä¹ ç”¨æ¥æ˜¾å¼åœ°å»ºæ¨¡ç‰¹å¾é€šé“é—´çš„ç›¸å…³æ€§ã€‚

å³ï¼šä½¿ç”¨ä¸€ä¸ªå…¨è¿æ¥å±‚ç¥ç»ç½‘ç»œï¼Œå¯¹ Squeeze ä¹‹åçš„ç»“æœè¿›è¡Œä¸€ä¸ªéçº¿æ€§å˜æ¢ã€‚

$$s = F_{ex}(z, W) = \sigma(g(z, W)) = \sigma(W_2 \delta(W_1 z)) $$

ï¼ˆ3ï¼‰Scale

æœ€åæ˜¯ä¸€ä¸ª Reweight çš„æ“ä½œï¼Œå°† Excitation çš„è¾“å‡ºçš„æƒé‡çœ‹åšæ˜¯è¿›è¿‡ç‰¹å¾é€‰æ‹©åçš„æ¯ä¸ªç‰¹å¾é€šé“çš„é‡è¦æ€§ï¼Œç„¶åé€šè¿‡ä¹˜æ³•é€é€šé“åŠ æƒåˆ°å…ˆå‰çš„ç‰¹å¾ä¸Šï¼Œå®Œæˆåœ¨é€šé“ç»´åº¦ä¸Šçš„å¯¹åŸå§‹ç‰¹å¾çš„é‡æ ‡å®šã€‚

$$\tilde{x} = F_{scale}(u_c, s_c) = s_c u_c$$

### SE Block å®ç°ç»†èŠ‚

ä½¿ç”¨ global average pooling ä½œä¸º Squeeze æ“ä½œï¼›

ç´§æ¥ç€ä¸¤ä¸ª Fully Connected å±‚ç»„æˆä¸€ä¸ª Bottleneck ç»“æ„å»å»ºæ¨¡é€šé“é—´çš„ç›¸å…³æ€§ï¼Œå¹¶è¾“å‡ºå’Œè¾“å…¥ç‰¹å¾åŒæ ·æ•°ç›®çš„æƒé‡ã€‚

é¦–å…ˆå°†ç‰¹å¾ç»´åº¦é™ä½åˆ°è¾“å…¥çš„ 1/16ï¼Œï¼ˆé™ä½è®¡ç®—é‡ï¼Œ16 æ˜¯å®è·µå¾—åˆ°çš„è¾ƒå¥½çš„è¶…å‚æ•°ï¼‰

ç„¶åç»è¿‡ ReLu æ¿€æ´»åå†é€šè¿‡ä¸€ä¸ª Fully Connected å±‚å‡å›åˆ°åŸæ¥çš„ç»´åº¦ã€‚ï¼ˆå¢åŠ éçº¿æ€§ï¼‰

é€šè¿‡ä¸€ä¸ª Sigmoid å‡½æ•°è·å¾— 0~1 ä¹‹é—´å½’ä¸€åŒ–çš„æƒé‡ã€‚

æœ€åé€šè¿‡ä¸€ä¸ª Scale çš„æ“ä½œæ¥å°†å½’ä¸€åŒ–åçš„æƒé‡åŠ æƒåˆ°æ¯ä¸ªé€šé“çš„ç‰¹å¾ä¸Šã€‚

SE Block å¯ä»¥åµŒå…¥åˆ°ç°åœ¨å‡ ä¹æ‰€æœ‰çš„ç½‘ç»œç»“æ„ä¸­ã€‚

### å®ä¾‹ Instantiations

é€šè¿‡åœ¨åŸå§‹ç½‘ç»œç»“æ„çš„ building block å•å…ƒä¸­åµŒå…¥ SE æ¨¡å—ï¼Œå¯ä»¥è·å¾—ä¸åŒç§ç±»çš„ SENetã€‚å¦‚ SE-BN-Inceptionã€SE-ResNetã€SE-ReNeXtã€SE-Inception-ResNet-v2 ç­‰ç­‰ã€‚

![](./20210122/3.png)

### SENet çš„å‚æ•°é‡å’Œè®¡ç®—é‡æƒ…å†µ

SENet é¢å¤–çš„æ¨¡å‹å‚æ•°éƒ½å­˜åœ¨äº Bottleneck è®¾è®¡çš„ä¸¤ä¸ª Fully Connected ä¸­ã€‚

ä»¥  SE-ResNet-50 å’Œ ResNet-50 ä¸ºä¾‹ï¼Œä»ç†è®ºä¸Šï¼ŒSE Block å¢é•¿çš„é¢å¤–è®¡ç®—é‡ä»…ä»…ä¸åˆ° 1%ã€‚

![](./20210122/4.png)

## SENet çš„è¡¨ç°

ResNet-50ã€ResNet-101ã€ResNet-152 å’ŒåµŒå…¥ SE æ¨¡å‹çš„ç»“æœã€‚SE-ResNets åœ¨å„ç§æ·±åº¦ä¸Šéƒ½è¿œè¿œè¶…è¿‡äº†å…¶å¯¹åº”çš„æ²¡æœ‰ SE çš„ç»“æ„ç‰ˆæœ¬çš„ç²¾åº¦ï¼Œè¿™è¯´æ˜æ— è®ºç½‘ç»œçš„æ·±åº¦å¦‚ä½•ï¼ŒSE æ¨¡å—éƒ½èƒ½å¤Ÿç»™ç½‘ç»œå¸¦æ¥æ€§èƒ½ä¸Šçš„å¢ç›Šã€‚

![](./20210122/9.jpg)

SE æ¨¡å—åµŒå…¥åˆ° ResNeXtã€BN-Inceptionã€Inception-ResNet-v2 ä¸Šå‡è·å¾—äº†ä¸è²çš„å¢ç›Šæ•ˆæœï¼ŒåŠ å…¥äº† SE æ¨¡å—çš„ç½‘ç»œæ”¶æ•›åˆ°æ›´ä½çš„é”™è¯¯ç‡ä¸Šã€‚

![](./20210122/5.png)

å…¶ä»–ï¼ˆCIFAR-10ã€CIFAR-100ã€Places365ã€COCOã€ImageNetï¼‰ï¼š

![](./20210122/6.png)

![](./20210122/7.png)

![](./20210122/8.png)

æœ€åï¼Œåœ¨ ILSVRC 2017 ç«èµ›ä¸­ï¼ŒSENet åœ¨æµ‹è¯•é›†ä¸Šè·å¾—äº† 2.251% Top-5 é”™è¯¯ç‡ã€‚å¯¹æ¯”äºå»å¹´ç¬¬ä¸€åçš„ç»“æœ 2.991%ï¼Œè·å¾—äº†å°†è¿‘ 25% çš„ç²¾åº¦æå‡ã€‚

> 2012~2017ï¼š ILSVRC 2017 ç«èµ›å† å†›ğŸ†ï¼š
>
> - 2012ï¼ŒAlexNetï¼štop-5: 15.32%
> - 2013ï¼ŒClarifaiï¼Œtop-5: 11.20%
> - 2014ï¼ŒGoogleNet v1ï¼Œtop-5: 6.67%
> - 2015ï¼ŒResNetï¼Œtop-5: 3.57%
> - 2016ï¼ŒTrimps-Soushenï¼ˆå…¬å®‰ä¸‰æ‰€ï¼‰ï¼Œtop-5: 2.99%
> - 2017ï¼Œ**SENet**ï¼Œtop-5: 2.25%

## âŒ SKNet

> ä¸€ä½œï¼šæç¿”ï¼Œåœ¨çŸ¥ä¹è°ˆ SKNetï¼š
>
> - ã€ŒSKNetâ€”â€”SENet å­ªç”Ÿå…„å¼Ÿç¯‡ã€ï¼š[https://zhuanlan.zhihu.com/p/59690223](https://zhuanlan.zhihu.com/p/59690223)

![](./20210122/10.png)

SKNet æˆ‘ç•™ä¸‹å‘¨è¿›è¡Œæ±‡æŠ¥ï¼ˆ1 æœˆ 29 æ—¥ï¼‰ã€‚

## å®éªŒ

å¯¹ ResNet50ã€SENet50 å’Œ SKNet 50 è¿›è¡Œç®€å•çš„æ¯”è¾ƒã€‚

æ•°æ®é›†é‡‡ç”¨ CIFAR-10ã€‚

é™¤äº† model ä¸åŒï¼Œä¸‰è€…å…¶ä»–è®­ç»ƒæ—¶çš„å‚æ•°éƒ½æ˜¯ä¸€è‡´çš„ã€‚

è®­ç»ƒæ—¶ä¿å­˜ checkpointï¼Œè¿™æ ·è°ƒå‚å°±ä¸ç”¨æ¯æ¬¡éƒ½ä» 0 å¼€å§‹è®­ç»ƒã€‚æœ‰ä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„æ¨¡å‹èƒ½å‡å°‘è®­ç»ƒéœ€è¦çš„æ—¶é—´ã€‚

```python
if args.resume:
    # Load checkpoint.
    print('==> Resuming from checkpoint..')
    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'
    checkpoint = torch.load('./checkpoint/ckpt.pth')
    net.load_state_dict(checkpoint['net'])
    best_acc = checkpoint['acc']
    start_epoch = checkpoint['epoch']
```

æ€»è®­ç»ƒ **200 epoch**ï¼Œæ¯ä¸¤ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpointï¼Œä½¿ç”¨ matplotlib ç»˜åˆ¶ rain_acc å’Œ test_acc æ›²çº¿ã€‚

ResNet50 è®­ç»ƒ 200 ä¸ª Epochã€‚ä¸‹å›¾ä¸º ResNet50 è®­ç»ƒç»“æŸï¼ŒTest Acc è¾¾åˆ° 95.41%ã€‚ 

![](./20210122/11.png)

æ¯ 2 ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpointï¼Œç”¨äºç»˜å›¾ã€‚ï¼ˆå¿˜è®°ä¿®æ”¹äº†ï¼Œå…¶å®ä¸éœ€è¦ä¿å­˜ net.state_dict çš„ï¼Œéå¸¸è€—ç©ºé—´ã€‚ï¼‰

> æˆ‘æŠŠ loss å¿˜è®°ä¿å­˜äº†ğŸŒšï¼Œloss æ›²çº¿ä¹Ÿå¾ˆé‡è¦ã€‚æˆ‘åªä¿å­˜äº† acc å’Œ epochã€‚

![](./20210122/12.png)

ä¸ä¿å­˜ `state_dict` ï¼Œåªä¿å­˜ lossã€epoch å’Œ accã€‚

![](./20210122/13.png)

### 1. ResNet50

> - å‚è€ƒä»£ç é“¾æ¥ï¼š[https://github.com/kuangliu/pytorch-cifar](<https://github.com/kuangliu/pytorch-cifar>)

```python
'''ResNet in PyTorch.
For Pre-activation ResNet, see 'preact_resnet.py'.
Reference:
[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
    Deep Residual Learning for Image Recognition. arXiv:1512.03385
'''
import torch
import torch.nn as nn
import torch.nn.functional as F


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion *
                               planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


def ResNet18():
    return ResNet(BasicBlock, [2, 2, 2, 2])

def ResNet34():
    return ResNet(BasicBlock, [3, 4, 6, 3])

def ResNet50():
    return ResNet(Bottleneck, [3, 4, 6, 3])

def ResNet101():
    return ResNet(Bottleneck, [3, 4, 23, 3])

def ResNet152():
    return ResNet(Bottleneck, [3, 8, 36, 3])

```

### 2. SENet50

åŸºäº ResNet50ï¼ŒåŠ å…¥ SE block å°±å¾—åˆ°äº† SENet50ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class BasicBlock(nn.Module):
    
    expansion = 4
    
    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

        # SE layers
        self.fc1 = nn.Conv2d(self.expansion*planes, self.expansion*planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear
        self.fc2 = nn.Conv2d(self.expansion*planes//16, self.expansion*planes, kernel_size=1)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.bn3(self.conv3(out))

        # Squeeze
        w = F.avg_pool2d(out, out.size(2))
        w = F.relu(self.fc1(w))
        w = F.sigmoid(self.fc2(w))
        # Excitation
        out = out * w  # New broadcasting feature from v0.2!

        out += self.shortcut(x)
        out = F.relu(out)
        return out

class SENet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(SENet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


def SENet50():
    return SENet(BasicBlock, [3,4,6,3])
```

SENet50 ç›¸æ¯”äº ResNet50ï¼Œç†è®ºä¸Šè®¡ç®—é‡å¢åŠ ä¸åˆ° 1%ï¼Œä½†æ˜¯æˆ‘å®é™…è®­ç»ƒæ—¶ï¼Œè€—æ—¶åŠ å€ï¼ˆ â‰ˆ ä¸€å¤©ä¸€å¤œï¼‰ã€‚

### 3. SKNet50

> SKNet çš„å®ç°å‚è€ƒï¼š[https://github.com/developer0hye/SKNet-PyTorch/blob/master/sknet.py](<https://github.com/developer0hye/SKNet-PyTorch/blob/master/sknet.py>)

### ç»˜åˆ¶ ResNet50ã€SENet50ã€SKNet50 çš„ Acc æ›²çº¿

å››ä¸ªç½‘ç»œï¼Œå„ 200 å„ Epoch çœŸçš„å¾ˆè€—æ—¶é—´ã€‚

```python
# è·å– acc éš epoch å¢åŠ çš„å€¼
total_epoch = 200

resnet_test_acc = []
resnet_train_acc = []
for i in range(0, total_epoch, 2):
    checkpoint = torch.load(resnet_path+"\\train_ckpt_epoch_%s.pth" % str(i))
    acc = checkpoint['acc_train']
    resnet_train_acc.append(acc)
    
    checkpoint = torch.load(resnet_path+"\\test_ckpt_epoch_%s.pth" % str(i))
    acc = checkpoint['acc_test']
    resnet_test_acc.append(acc)
```

ï¼ˆ1ï¼‰ResNet50ï¼šAcc éš epoch å˜æ¢æ›²çº¿

![](./20210122/resnet50-acc-curve.png)

ResNet è¡¨ç°å¾—é‚£ä¹ˆå¥½ï¼Œå€’æ˜¯è®©æˆ‘æ„Ÿè§‰å¾ˆå¥‡æ€ªã€‚ResNet è®ºæ–‡ä¸­ï¼Œåœ¨ CIFAR-10 æ•°æ®é›†ä¸Šï¼Œæµ‹è¯•å¾—åˆ° ResNet44 çš„è¡¨ç°ä¸º 92.83%ï¼ŒReNet56 çš„è¡¨ç°ä¸º 93.03%ã€‚

å¦‚ä¸Šå®ç°çš„ ResNet50 åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ç«Ÿç„¶è¾¾åˆ°äº† 95.45%ã€‚

ï¼ˆ2ï¼‰SENet50

![](./20210122/senet50-acc-curve.png)

- SENet18

![](./20210122/senet18-acc-curve.png)

ï¼ˆ3ï¼‰SKNet50

![](./20210122/sknet50-acc-curve.png)

æ•´åˆåˆ°ä¸€å¼ å›¾ç‰‡ä¸Šï¼Œæ–¹ä¾¿ç›´è§‚çš„è¿›è¡Œæ¯”è¾ƒã€‚

![](./20210122/integration.png)

ä»ç†è®ºä¸Šåˆ†æï¼Œå„ç½‘ç»œçš„è¡¨ç°æƒ…å†µåº”è¯¥æ˜¯ï¼š

ResNet50 < SENet50 < SKNet50.

ä½†æ˜¯æˆ‘å¤ç°çš„å®è·µç»“æœä¸ºï¼š ResNet50(95.45%) > SENet50(95.05%) > SKNet50(89.81)ã€‚

é—®é¢˜å‡ºç°åœ¨å“ªé‡Œï¼Ÿ

## å‚æ•°è°ƒä¼˜

è¿›è¡Œä¸€ç³»åˆ—çš„å‚æ•°è°ƒä¼˜ï¼Œç›®æ ‡æ˜¯å®ç° ResNet50 < SENet50 < SKNet50.

å¾…è§£å†³çš„é—®é¢˜ï¼š

- ï¼ˆ1ï¼‰ResNet50 è¡¨ç°å¾—é‚£ä¹ˆå¥½ï¼Œæœ‰é—®é¢˜å—ï¼Ÿ
- ï¼ˆ2ï¼‰SENet18 ç«Ÿç„¶ä¼˜äº SENet50ï¼Œè¿™ä¸ªå¾ˆæœ‰é—®é¢˜ï¼

![](./20210122/senet50-and-senet18.png)

- ï¼ˆ3ï¼‰SKNet50 ç«Ÿç„¶æ²¡æœ‰ä¸Š 90%ï¼Œè¿™ä¸ªå¾ˆæœ‰é—®é¢˜ï¼ä¸‹å‘¨å†è§£å†³å§ã€‚

